{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrAvLaHh7OlU"
      },
      "source": [
        "# Python Notebook for ML Weather Forecasting Project\n",
        "\n",
        " Link to data: https://www.ecad.eu/dailydata/\n",
        "\n",
        " Project directory: https://drive.google.com/drive/folders/1gnI99uHnB-pbb-bsX1302IQZVNudKgUm?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NenEviK58nbU"
      },
      "source": [
        " ## **Section I: Data retrieval**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FUu5gX6fmoEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6f5b49-8ad3-4872-969b-2bd80e111802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvDF532YDB_o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.offsetbox import AnchoredText\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rBfJgHy8vei"
      },
      "outputs": [],
      "source": [
        "features_dict = {\n",
        "    'cloud_cover': 'CC', # cloud cover in oktas\n",
        "    'glob_rad': 'QQ', # global radiation in W/m2\n",
        "    'precip': 'RR', # precipitation amount in 0.1 mm\n",
        "    'mean_temp': 'TG', # mean temperature in 0.1\n",
        "    'min_temp': 'TN', # minimum temperature in 0.1\n",
        "    'max_temp': 'TX', # maximum temperature in 0.1\n",
        "    'wind_speed': 'FG', # wind speed in 0.1 m/s\n",
        "    'humidity': 'HU', # humidity in 1 %\n",
        "    'sea_level_pressure': 'PP', # sea level pressure in 0.1 hPa\n",
        "    'sunshine': 'SS' # sunshine in 0.1 Hours\n",
        "}\n",
        "\n",
        "city = 'Valencia'\n",
        "features = ['glob_rad',\n",
        "            'precip',\n",
        "            'mean_temp',\n",
        "            'min_temp',\n",
        "            'max_temp',\n",
        "            'cloud_cover',\n",
        "            'wind_speed',\n",
        "            'humidity',\n",
        "            'sea_level_pressure',\n",
        "            'sunshine']\n",
        "\n",
        "project_dir = 'drive/MyDrive/ECIU ML Project Submission/Colab Notebooks/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g83AhhqxUwq"
      },
      "outputs": [],
      "source": [
        "new_import = False\n",
        "\n",
        "if new_import:\n",
        "\n",
        "  total_data = pd.DataFrame(columns=['DATE'])\n",
        "\n",
        "  for i, feat in enumerate(features):\n",
        "    feat_ID = features_dict[feat]\n",
        "    files_list = os.listdir(project_dir + city)\n",
        "    file_name = [f for f in files_list if feat_ID in f][0]\n",
        "    f = open(project_dir + city + '/' + file_name,'r')\n",
        "    for line in f:\n",
        "      if 'SOUID,    DATE,' not in line:\n",
        "        pass\n",
        "      else: # this is where the data of the .txt file starts\n",
        "        header = line\n",
        "        data = f.readlines()[:] # .readlines() starts from the current index\n",
        "        data = pd.read_csv(io.StringIO(''.join(data)), delimiter=',',\n",
        "                          header=None,\n",
        "                          names=header[:-1].replace(' ','').split(','))\n",
        "        # source and station ID not needed\n",
        "        data = data.drop(['SOUID', 'STAID'], axis=1, errors='ignore')\n",
        "        data['DATE'] = pd.to_datetime(data['DATE'], format='%Y%m%d')\n",
        "        # set measurements to NaN where quality ID == 9\n",
        "        data.loc[data['Q_'+feat_ID] == 9, feat_ID] = np.nan\n",
        "        data = data.drop(['Q_'+feat_ID], axis=1) # quality ID not needed anymore\n",
        "        data = data.rename(columns={feat_ID: feat})\n",
        "        data = data.set_index('DATE') # setting date as index of df\n",
        "        data = data.resample('W').mean()\n",
        "        # index of the total dataframe is set to the index of the 1st feature\n",
        "        if i == 0:\n",
        "          total_data['DATE'] = data.index\n",
        "          total_data = total_data.set_index('DATE')\n",
        "        total_data = pd.concat([total_data, data], axis=1, join='inner')\n",
        "        break\n",
        "\n",
        "  # saving data to csv\n",
        "  # total_data.to_csv(project_dir+city+'/TOTAL.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQvZ8tlCrZxP"
      },
      "source": [
        "##**Section II: Data visualization and feature selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDu777Sdr_Xx"
      },
      "outputs": [],
      "source": [
        "total_data = pd.read_csv(project_dir+city+'/TOTAL.csv')\n",
        "total_data['DATE'] = pd.to_datetime(total_data['DATE'])\n",
        "total_data = total_data.set_index('DATE')\n",
        "# dropping wind speed due to its incompleteness\n",
        "total_data = total_data.drop(['wind_speed'], axis=1)\n",
        "\n",
        "# the data from around 1955 to 2003 seems to be most complete\n",
        "start = '01-01-1995'\n",
        "end = '01-01-2003'\n",
        "sub_data = total_data[start:end]\n",
        "\n",
        "N = 3\n",
        "pred_target = 'mean_temp'\n",
        "shifted_data = pd.DataFrame(sub_data[pred_target])\n",
        "\n",
        "def derive_nth_day_feature(original_df, shifted_df, feature, N):\n",
        "    days = original_df.shape[0]\n",
        "    nth_prior_measurements = [np.nan]*N + [\n",
        "      original_df[feature][i-N] for i in range(N, days)]\n",
        "    col_name = \"{}_{}\".format(feature, N)\n",
        "    shifted_df[col_name] = nth_prior_measurements\n",
        "\n",
        "for feat in sub_data.columns:\n",
        "  for n in range(1,N+1):\n",
        "    shifted_df = derive_nth_day_feature(original_df=sub_data,\n",
        "                                        shifted_df=shifted_data,feature=feat,\n",
        "                                        N=n)\n",
        "\n",
        "# drop NaN values that were created due to shifting\n",
        "shifted_data = shifted_data.dropna()\n",
        "# shifted_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIfZrn3B4K7l"
      },
      "outputs": [],
      "source": [
        "# plot data histograms\n",
        "\n",
        "for i, col in enumerate(shifted_data.columns):\n",
        "  plt.figure(i, figsize=(12, 6), dpi=80)\n",
        "  shifted_data[col].hist(bins=20)\n",
        "  plt.title('Distribution of %s' % col)\n",
        "  plt.xlabel(col)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ4qffHHFGbK"
      },
      "source": [
        "Calculating the Pearson correlation coefficient (measure for linear correlation between two variables):\n",
        "\n",
        "$$r_{xy}={\\frac {\\sum _{i=1}^{m}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{{\\sqrt {\\sum _{i=1}^{m}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{i=1}^{m}(y_{i}-{\\bar {y}})^{2}}}}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2lHtgOY_zNx"
      },
      "outputs": [],
      "source": [
        "corr_coeffs = shifted_data.corr()[['mean_temp']].sort_values('mean_temp')\n",
        "print(corr_coeffs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqZ-wM5aBPrW"
      },
      "outputs": [],
      "source": [
        "# removing features with |r| < 0.3\n",
        "to_remove = ('humidity', 'precip', 'sea_level_pressure', 'cloud_cover')\n",
        "\n",
        "# predictors will be all the other features\n",
        "predictors = [f for f in shifted_data.columns if not f.startswith(to_remove)]\n",
        "predictors.remove(pred_target)\n",
        "predictor_data = shifted_data[predictors]\n",
        "target_data = shifted_data[pred_target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6YNcCSw0eoW"
      },
      "outputs": [],
      "source": [
        "# plot correlation diagrams\n",
        "\n",
        "Ncols = N\n",
        "Nrows = len(predictors)//N\n",
        "# manually set the parameters of the figure to and appropriate size\n",
        "plt.rcParams['figure.figsize'] = [16, 20]\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "\n",
        "\n",
        "# call subplots specifying the grid structure\n",
        "fig, axes = plt.subplots(nrows=Nrows, ncols=Ncols, sharey=True)\n",
        "\n",
        "# rearrange data into a 2D array\n",
        "arr = np.array(predictors).reshape(Nrows, Ncols)\n",
        "\n",
        "# use enumerate to loop over the arr 2D array of rows and columns\n",
        "# and create scatter plots of each mean_temp vs each feature\n",
        "for row, col_arr in enumerate(arr):\n",
        "    for col, feat in enumerate(col_arr):\n",
        "        axes[row, col].scatter(shifted_data[feat], shifted_data[pred_target])\n",
        "        axes[row, col].grid(which='both')\n",
        "        anchored_text = AnchoredText('$r=%.2f$'%corr_coeffs[pred_target][feat],\n",
        "                                     loc='lower right')\n",
        "        axes[row, col].add_artist(anchored_text)\n",
        "        if col == 0:\n",
        "            axes[row, col].set(xlabel=feat, ylabel=pred_target)\n",
        "            axes[row, col].set_xticks([])\n",
        "            axes[row, col].set_yticks([])\n",
        "        else:\n",
        "            axes[row, col].set(xlabel=feat)\n",
        "            axes[row, col].set_xticks([])\n",
        "            axes[row, col].set_yticks([])\n",
        "plt.tight_layout(pad=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfZZ98Skm8qo"
      },
      "source": [
        "##**Section III: Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMAAETLcAsCZ"
      },
      "source": [
        "Cost function:\n",
        "\n",
        "$$J(\\mathbf{\\theta})=\\frac{1}{2m}\\sum_{i=1}^{m}\\bigl[h_{\\theta}(\\mathbf{x}_i)-y_i\\bigr]^2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lBofcahnMJJ"
      },
      "outputs": [],
      "source": [
        "def computeCost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Take the numpy arrays X, y, theta and return the cost function J for this\n",
        "    theta.\n",
        "\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    h = X.dot(theta)\n",
        "    square_err = (h - y) ** 2\n",
        "    J = 1 / (2 * m) * sum(square_err)\n",
        "\n",
        "    return J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ooKdQnpn7v"
      },
      "source": [
        "Gradient descent:\n",
        "$\\mathbf{\\theta}^{(k+1)} = \\mathbf{\\theta}^{(k)} - \\alpha\\nabla_{\\theta}J(\\mathbf{\\theta}^{(k)}) = \\mathbf{\\theta}^{(k)} - \\alpha\\frac{1}{m} \\mathbf{X}^\n",
        "\\mathsf{T}(\\mathbf{h}_{\\mathbf{\\theta}}^{(k)}-\\mathbf{y})$\n",
        "\n",
        "where\n",
        "\n",
        "$\\mathbf{h}_{\\mathbf{\\theta}} = \\begin{bmatrix}h_{\\mathbf{\\theta}}(\\mathbf{x}_1)\\\\h_{\\mathbf{\\theta}}(\\mathbf{x}_2)\\\\\\cdots\\\\h_{\\mathbf{\\theta}}(\\mathbf{x}_m)\\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk51JijYwGdS"
      },
      "outputs": [],
      "source": [
        "def gradientDescent(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Take numpy arrays X, y and theta and update theta by taking num_iters\n",
        "    gradient steps with learning rate alpha\n",
        "\n",
        "    Return: theta and the list of the cost of theta (J_history) during each\n",
        "    iteration\n",
        "    \"\"\"\n",
        "\n",
        "    m = len(y)\n",
        "    J_history = []\n",
        "\n",
        "    for k in range(num_iters):\n",
        "      h = X.dot(theta)\n",
        "      # vectorized way to compute all gradients simultaneously\n",
        "      grad = X.transpose().dot(h - y)\n",
        "      descent = (alpha / m) * grad\n",
        "      theta = theta - descent\n",
        "      J_history.append(computeCost(X, y, theta))\n",
        "\n",
        "    return theta, J_history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "if 'intercept' not in predictor_data.columns:\n",
        "  predictor_data.insert(0, 'intercept', 1)\n",
        "\n",
        "normalized = False\n",
        "\n",
        "if not normalized:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(predictor_data,\n",
        "                                                    target_data,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=5)\n",
        "else: # option to use feature normalization\n",
        "  scaler = StandardScaler()\n",
        "  normalized_data = pd.DataFrame(\n",
        "      scaler.fit_transform(predictor_data, target_data),\n",
        "      index = predictor_data.index,\n",
        "      columns = predictor_data.columns.delete(0))\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(normalized_data,\n",
        "                                                    target_data,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=5)"
      ],
      "metadata": {
        "id": "ka6tSSGQtLGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz631jexz42p"
      },
      "outputs": [],
      "source": [
        "m = X_train.shape[0]\n",
        "n = X_train.shape[1]\n",
        "\n",
        "# initialize the fitting parameters theta to 0 (np.zeros)\n",
        "\n",
        "theta = np.zeros((n, 1))\n",
        "\n",
        "theta, J_history = gradientDescent(X=X_train.values,\n",
        "                                   y=y_train.values.reshape(m, 1),\n",
        "                                   theta=theta, alpha=1e-7, num_iters=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03tqZ98ZGdOJ"
      },
      "outputs": [],
      "source": [
        "# print model function h(x)\n",
        "\n",
        "print('h(x) = %.2f' % theta[0])\n",
        "for i, pred in enumerate(predictors):\n",
        "  print('\\t+ %.2f * %s' % (theta[i+1], pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvbm68WR_qa5"
      },
      "outputs": [],
      "source": [
        "# plot convergence of cost function\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "plt.xlabel('Number of iterations $k$')\n",
        "plt.ylabel('Cost function $J(theta)$')\n",
        "plt.grid('both')\n",
        "plt.plot(J_history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute predictions for y_test using the model\n",
        "\n",
        "predictions = X_test.dot(theta).values\n",
        "\n",
        "# Evaluate the prediction performance of the model\n",
        "from sklearn.metrics import mean_squared_error, median_absolute_error\n",
        "\n",
        "MSE = 0.1 * mean_squared_error(y_test.values, predictions.ravel())\n",
        "MAE = 0.1 * median_absolute_error(y_test.values, predictions.ravel())\n",
        "\n",
        "print(\"The Mean Squared Error: %.2f degrees celsius\" % MSE)\n",
        "print(\"The Median Absolute Error: %.2f degrees celsius\" % MAE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b3VZODmwdPd",
        "outputId": "6180c3f4-f4bc-49b6-b2ca-bd194e9993cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mean Squared Error: 56.88 degrees celsius\n",
            "The Median Absolute Error: 1.49 degrees celsius\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot distribution of absolute prediction error\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "plt.xlabel('Absolute Error in °C')\n",
        "plt.ylabel('# of predictions')\n",
        "plt.hist(0.1*abs(y_test.values - predictions.ravel()), bins=20)"
      ],
      "metadata": {
        "id": "je4RYCB06hye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0DlTag51h97"
      },
      "outputs": [],
      "source": [
        "# plot correlation diagrams now including the predictions\n",
        "\n",
        "Ncols = N\n",
        "Nrows = len(predictors)//N\n",
        "# manually set the parameters of the figure to and appropriate size\n",
        "plt.rcParams['figure.figsize'] = [16, 22]\n",
        "\n",
        "# call subplots specifying the grid structure we desire and that\n",
        "# the y axes should be shared\n",
        "fig, axes = plt.subplots(nrows=Nrows, ncols=Ncols, sharey=True)\n",
        "\n",
        "# rearrange predictor names into 2D array\n",
        "arr = np.array(predictors).reshape(Nrows, Ncols)\n",
        "\n",
        "# create dataframe that includes linspaces for all predictors from min to max\n",
        "min_max_df = pd.DataFrame()\n",
        "min_max_df['intercept'] = np.ones(50)\n",
        "for pred in predictors:\n",
        "  min_max_df[pred] = np.linspace(min(predictor_data[pred]),\n",
        "                                 max(predictor_data[pred]))\n",
        "\n",
        "predictions = min_max_df.dot(theta).values\n",
        "\n",
        "# loop over 2D array and create scatter plots of mean_temp vs each feature\n",
        "for row, col_arr in enumerate(arr):\n",
        "    for col, feat in enumerate(col_arr):\n",
        "        axes[row, col].scatter(predictor_data[feat], target_data)\n",
        "        axes[row, col].plot(min_max_df[feat], predictions, color='r')\n",
        "        if col == 0:\n",
        "            axes[row, col].set(xlabel=feat, ylabel=pred_target)\n",
        "        else:\n",
        "            axes[row, col].set(xlabel=feat)\n",
        "plt.tight_layout(pad=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0gnNSppdPPp"
      },
      "source": [
        "## **Section IV: Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import explained_variance_score, mean_squared_error, \\\n",
        "median_absolute_error\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "K8gUtS5lwXy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training set and a temporary set\n",
        "X_train, X_tmp, y_train, y_tmp = train_test_split(shifted_data,\n",
        "                                                  target_data,\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state=5)\n",
        "\n",
        "# take the remaining 20% of data in X_tmp, y_tmp and split them evenly\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_tmp, y_tmp, test_size=0.5,\n",
        "                                                random_state=5)\n",
        "\n",
        "print(\"Training instances   {}, Training features   {}\".format(\n",
        "    X_train.shape[0], X_train.shape[1]))\n",
        "print(\"Validation instances {}, Validation features {}\".format(\n",
        "    X_val.shape[0], X_val.shape[1]))\n",
        "print(\"Testing instances    {}, Testing features    {}\".format(\n",
        "    X_test.shape[0], X_test.shape[1]))"
      ],
      "metadata": {
        "id": "YhMGyD3twcYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the ANN\n",
        "feature_cols = [tf.feature_column.numeric_column(col) for col in\n",
        "                predictor_data.columns]\n",
        "regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols,\n",
        "                                      hidden_units=[50, 50],\n",
        "                                      model_dir=project_dir+'ANN_data')"
      ],
      "metadata": {
        "id": "JqDXhjvQwcBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(X, y=None, num_epochs=None, shuffle=True, batch_size=1000):\n",
        "  # we have to drop the datetime index by adding .reset_index(drop=True)\n",
        "  X = X.reset_index(drop=True)\n",
        "  if y is not None:\n",
        "    y = y.reset_index(drop=True)\n",
        "  return tf.compat.v1.estimator.inputs.pandas_input_fn(x=X, y=y,\n",
        "                                                       num_epochs=num_epochs,\n",
        "                                                       shuffle=shuffle,\n",
        "                                                       batch_size=batch_size)"
      ],
      "metadata": {
        "id": "DkvAYkQSwb7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training of the ANN; a lot of output will be printed in the console!\n",
        "\n",
        "evaluations = []\n",
        "STEPS = 100\n",
        "for i in range(50):\n",
        "  print('\\nIteration: ' + str(i+1))\n",
        "  regressor.train(input_fn=input_fn(X_train, y=y_train,\n",
        "                                    batch_size=X_train.shape[0]//2),\n",
        "                  steps=STEPS)\n",
        "  evaluations.append(regressor.evaluate(input_fn=input_fn(X_val,\n",
        "                                                          y_val,\n",
        "                                                          num_epochs=1,\n",
        "                                                          shuffle=False)))"
      ],
      "metadata": {
        "id": "U_N9WdvgwbOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually set the parameters of the figure\n",
        "plt.rcParams['figure.figsize'] = [12, 6]\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "\n",
        "loss_values = [ev['average_loss'] for ev in evaluations]\n",
        "training_steps = [ev['global_step'] for ev in evaluations]\n",
        "\n",
        "# plot convergence of the total squared error\n",
        "plt.plot(training_steps, loss_values)\n",
        "plt.xlabel('Training steps (Epochs = steps / 2)')\n",
        "plt.ylabel('Total squared error in °C')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DJf-_6opyr6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use ANN to predict mean temperatures\n",
        "pred = regressor.predict(input_fn=input_fn(X_test, num_epochs=1, shuffle=False))\n",
        "predictions = np.array([p['predictions'][0] for p in pred])\n",
        "\n",
        "MSE = 0.1 * mean_squared_error(y_test.values, predictions.ravel())\n",
        "MAE = 0.1 * median_absolute_error(y_test.values, predictions.ravel())\n",
        "\n",
        "print(\"\\nThe Mean Squared Error: %.2f degrees Celcius\" % MSE)\n",
        "print(\"The Median Absolute Error: %.2f degrees Celcius\" % MAE)"
      ],
      "metadata": {
        "id": "B1edUyuO5qQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot distribution of absolute prediction error\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "plt.xlabel('Absolute Error in °C')\n",
        "plt.ylabel('# of predictions')\n",
        "plt.hist(0.1*abs(y_test.values - predictions.ravel()), bins=20)"
      ],
      "metadata": {
        "id": "6TYTfRU35qIV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}